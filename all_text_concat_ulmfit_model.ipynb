{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_text_concat_ulmfit_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1ku2GYgICBSrBm/cA5BVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanbanerjee32/feedback-prize-effectiveness/blob/main/all_text_concat_ulmfit_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vAwrF3S24Hzn"
      },
      "outputs": [],
      "source": [
        "# install fastkaggle if not available\n",
        "!pip install -Uq fastai\n",
        "try: import fastkaggle\n",
        "except ModuleNotFoundError:\n",
        "    !pip install -Uq fastkaggle\n",
        "\n",
        "# !pip install -Uq 'timm>=0.6.2.dev'\n",
        "!pip install -Uq pynvml\n",
        "from fastkaggle import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import fastai\n",
        "fastai.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J0E4FZde4QB3",
        "outputId": "f881dc1b-85fc-4278-d6c9-293452c18036"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.7.9'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config depending on whether this is running on kaggle or collab\n",
        "# is_colab = True\n",
        "is_colab = not os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
        "comp = 'feedback-prize-effectiveness'\n",
        "if is_colab:\n",
        "    model_save_path = Path('/content/'+comp+'_out/models')\n",
        "else:\n",
        "    model_save_path = Path('/kaggle/working/'+comp) #+'/models')"
      ],
      "metadata": {
        "id": "LwaMmLpipyJd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import colab libraries\n",
        "if is_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOjHaaj64SAU",
        "outputId": "2e4c01ae-5f2e-4aaa-dce5-7e23f9b42f6f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so lets move it there.\n",
        "if is_colab:\n",
        "    !mkdir ~/.kaggle\n",
        "    !cp /content/drive/MyDrive/Kaggle_api_auth/kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "Rv1FT2oS4X_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b59756c-d2e6-4fbe-9064-432bcc56b978"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "if is_colab:\n",
        "    !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "SM6uBIQ34eqk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = setup_comp(comp)\n",
        "path"
      ],
      "metadata": {
        "id": "Ikwoyt764icQ",
        "outputId": "e0f85ca4-1b44-4c6f-d1de-99fbfc54827e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('feedback-prize-effectiveness')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.text.all import *\n",
        "set_seed(32)"
      ],
      "metadata": {
        "id": "Xihu0wcU4sSD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language model for original text"
      ],
      "metadata": {
        "id": "zigZQFGkStlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell fastai TextDataLoader functions are overridden to allow taking output \n",
        "# directory as input so that the toknised data is not written in data input directory\n",
        "# in Kaggle, data input directory is read-only\n",
        "from fastai.text.core import _tokenize_files\n",
        "@delegates(_tokenize_files)\n",
        "def tokenize_folderOP(path, extensions=None, folders=None,\n",
        "                      output_dir=None, skip_if_exists=True, **kwargs):\n",
        "    \"\"\"replacement of fastai.text.core.tokenize_folder so that it passes\n",
        "     output_dir as input to _tokenize_files\"\"\"\n",
        "    path,extensions = Path(path),ifnone(extensions, ['.txt'])\n",
        "    files = get_files(path, extensions=extensions, recurse=True, folders=folders)\n",
        "    def _f(i,output_dir): return output_dir/files[i].relative_to(path)\n",
        "    return _tokenize_files(_f, files, path, output_dir, \n",
        "                           skip_if_exists=skip_if_exists, **kwargs)\n",
        "\n",
        "class TokenizerOP(Tokenizer):\n",
        "    \"\"\"Wrapper class for fastai.text.core.Tokenizer class to override\n",
        "    from_folder method to take output_dir as input and call the overridden\n",
        "    method tokenize_folderOP\"\"\"\n",
        "    def __init__(self, tok, rules=None, counter=None,\n",
        "                 lengths=None, mode=None, sep=' '):\n",
        "        super().__init__(tok, rules=rules, counter=counter,\n",
        "                 lengths=lengths, mode=mode, sep=sep)\n",
        "    \n",
        "    @classmethod\n",
        "    @delegates(tokenize_folderOP, keep=True)\n",
        "    def from_folder(cls, path, tok=None, rules=None, output_dir = None, **kwargs):\n",
        "        path = Path(path)\n",
        "        if tok is None: tok = WordTokenizer()\n",
        "        output_dir = tokenize_folderOP(path, tok=tok, rules=rules, \n",
        "                                       skip_if_exists = False, \n",
        "                                       output_dir = output_dir, **kwargs)\n",
        "        res = cls(tok, counter=load_pickle(output_dir/fn_counter_pkl),\n",
        "                  lengths=load_pickle(output_dir/fn_lengths_pkl),\n",
        "                  rules=rules, mode='folder')\n",
        "        res.path,res.output_dir = path,output_dir\n",
        "        return res\n",
        "\n",
        "\n",
        "class TextBlockOP(TextBlock):\n",
        "    \"Overriding TextBlock for user defined output dir\"\n",
        "    def __init__(self, tok_tfm, vocab=None, is_lm=False,\n",
        "                 seq_len=72, backwards=False, **kwargs):\n",
        "        super().__init__(tok_tfm, vocab=vocab, is_lm=is_lm,\n",
        "                 seq_len=seq_len, backwards=backwards, **kwargs)\n",
        "    @classmethod\n",
        "    @delegates(TokenizerOP.from_folder, keep=True)\n",
        "    def from_folder(cls, path, vocab=None, is_lm=False, seq_len=72,\n",
        "                    backwards=False, min_freq=3, max_vocab=60000,output_dir =None,\n",
        "                    **kwargs):\n",
        "        \"Build a `TextBlock` from a `path` - calls TokenizerOP and provides output_dir as input\"\n",
        "        return cls(TokenizerOP.from_folder(path, output_dir = output_dir, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len,\n",
        "                   backwards=backwards, min_freq=min_freq, max_vocab=max_vocab)\n",
        "        \n",
        "class TextDataLoadersOP(TextDataLoaders):\n",
        "    \"Basic wrapper around several `DataLoader`s with factory methods for NLP problems\"\n",
        "    @classmethod\n",
        "    @delegates(DataLoaders.from_dblock)\n",
        "    def from_folder(cls, path, train='train', valid='valid', valid_pct=None,\n",
        "                    seed=None, vocab=None, text_vocab=None, is_lm=False,\n",
        "                    tok_tfm=None, seq_len=72, splitter=None,\n",
        "                    backwards=False, output_dir =None, **kwargs):\n",
        "        \"This is to override same method from TextDataLoaders to accept and pass output_dir as input\"\n",
        "        if splitter is None:\n",
        "            splitter = GrandparentSplitter(train_name=train,\n",
        "                                           valid_name=valid) if valid_pct is None else RandomSplitter(valid_pct,\n",
        "                                                                                                      seed=seed)\n",
        "        blocks = [TextBlockOP.from_folder(path, text_vocab, is_lm, seq_len, backwards,\n",
        "                                          tok=tok_tfm, output_dir = output_dir)]\n",
        "        if not is_lm: blocks.append(CategoryBlock(vocab=vocab))\n",
        "        get_items = partial(get_text_files, folders=[train,valid]) if valid_pct is None else get_text_files\n",
        "        dblock = DataBlock(blocks=blocks,\n",
        "                           get_items=get_items,\n",
        "                           splitter=splitter,\n",
        "                           get_y=None if is_lm else parent_label)\n",
        "        return cls.from_dblock(dblock, path, path=path, seq_len=seq_len, **kwargs)\n"
      ],
      "metadata": {
        "id": "a32FvSRrp02R"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_path = model_save_path / 'train_tok'\n",
        "tok_path.mkdir(parents=True, exist_ok=True)\n",
        "dls_lm = TextDataLoadersOP.from_folder(path / 'train', is_lm=True,\n",
        "                                       valid_pct=0.1, output_dir = tok_path)\n",
        "# TextDataLoaders.from_folder(path / 'train', is_lm=True, valid_pct=0.1, )\n",
        "dls_lm.show_batch(max_n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "mp9oSroFS1Ub",
        "outputId": "3170c5a3-5d84-46ed-897a-82a3be59590b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ""
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj my principal says we all have to participate in at least one after school activity . i totally agree with him . xxmaj one of the reasons why is so we can make more friends and be more comfortable xxunk people . xxmaj the reason why we need to have friends and be more comfortable around people is because friends are always a important part of a persons life .</td>\n",
              "      <td>xxmaj my principal says we all have to participate in at least one after school activity . i totally agree with him . xxmaj one of the reasons why is so we can make more friends and be more comfortable xxunk people . xxmaj the reason why we need to have friends and be more comfortable around people is because friends are always a important part of a persons life . xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>. \\n\\n xxmaj those are three reasons why i would talk to multiple different source 's and get there advice instead of just talking to only one source and getting a xxunk amount of advice compared to the amount of advice i would get talking to multiple source 's . xxbos xxmaj facial action coding xxunk should be used because it is a good way to help students and people , xxmaj</td>\n",
              "      <td>\\n\\n xxmaj those are three reasons why i would talk to multiple different source 's and get there advice instead of just talking to only one source and getting a xxunk amount of advice compared to the amount of advice i would get talking to multiple source 's . xxbos xxmaj facial action coding xxunk should be used because it is a good way to help students and people , xxmaj first</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i do nt want that in our country . xxmaj you may be thinking what do i mean by i do nt want that in our country what i mean by that is that if there is problems i do nt want that in our country cause are country is already free so i do nt want it to have problems . xxmaj and then if the xxmaj united xxmaj states fight</td>\n",
              "      <td>do nt want that in our country . xxmaj you may be thinking what do i mean by i do nt want that in our country what i mean by that is that if there is problems i do nt want that in our country cause are country is already free so i do nt want it to have problems . xxmaj and then if the xxmaj united xxmaj states fight it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not to your control . but i was hoping that everyone could be happy . but both does sound alot . if we ca n't do both we can at least do one that will benefit everyone . i know i am just a student but we can make a big difference in the community . we could probably make it better . you decide which one is better . and hopefully</td>\n",
              "      <td>to your control . but i was hoping that everyone could be happy . but both does sound alot . if we ca n't do both we can at least do one that will benefit everyone . i know i am just a student but we can make a big difference in the community . we could probably make it better . you decide which one is better . and hopefully let</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\n xxmaj next , the figure similar to a \" face \" is to big to be a real face . xxmaj for xxunk enormous head nearly two miles from end to end . xxunk ) xxmaj this quote shows that the head is to big to be a real face . xxmaj like , what face is two miles long from end to end ? \\n\\n xxmaj last but not least</td>\n",
              "      <td>xxmaj next , the figure similar to a \" face \" is to big to be a real face . xxmaj for xxunk enormous head nearly two miles from end to end . xxunk ) xxmaj this quote shows that the head is to big to be a real face . xxmaj like , what face is two miles long from end to end ? \\n\\n xxmaj last but not least ,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dump vocab that will be required for inference script\n",
        "with open(model_save_path / 'dls_lm_vocab.pickle', 'wb') as b:\n",
        "    pickle.dump(dls_lm.vocab,b)"
      ],
      "metadata": {
        "id": "5nDI4aAwaVp6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_lm = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()"
      ],
      "metadata": {
        "id": "3bcGSEuCTRrk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_lm.fit_one_cycle(1, 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "ohXZDqkFTYrm",
        "outputId": "9da2e008-3262-4a27-f526-fdd4b467bd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00&lt;?]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='292' class='' max='401' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      72.82% [292/401 00:59&lt;00:22 3.7902]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn_lm.unfreeze()\n",
        "learn_lm.fit_one_cycle(10, 1e-3)"
      ],
      "metadata": {
        "id": "KE1k8a1xTgNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path.mkdir(parents=True, exist_ok=True)\n",
        "if iskaggle:\n",
        "    # hack to save encoder in a writable location\n",
        "    learn_lm.path = model_save_path\n",
        "    learn_lm.save_encoder('finetuned_enc')\n",
        "    # learn_lm.save('finetuned_lm')\n",
        "else:\n",
        "    learn_lm.save_encoder(model_save_path / 'finetuned_enc')"
      ],
      "metadata": {
        "id": "VBxJGBQCXhei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text classification"
      ],
      "metadata": {
        "id": "_WyXQ4ySSyUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def file_read(file_path):\n",
        "    with open(file_path, 'r') as _f: \n",
        "        all_content = _f.read()\n",
        "    return all_content"
      ],
      "metadata": {
        "id": "DaZzI-zTqNSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process text - add all columns \n",
        "df = pd.read_csv(path/'train.csv')\n",
        "df['essay_text'] = df['essay_id'].apply(lambda x: file_read(path / 'train' / f'{x}.txt'))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "XY6PVulLoZFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decide sequence length\n",
        "df['seq_length_essay'] = [len(txt.split()) for txt in df['essay_text'].tolist()]\n",
        "df['seq_length_dis'] = [len(txt.split()) for txt in df['discourse_text'].tolist()]\n",
        "df['seq_length_essay'].describe(), df['seq_length_dis'].describe()"
      ],
      "metadata": {
        "id": "k__FA3GVwqeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace discorse text within the context text as __MASKED__\n",
        "df['masked_ess_txt'] = df[['essay_text','discourse_text']].apply(lambda row: row.essay_text.strip().replace(row.discourse_text.strip(),\n",
        "                                                                                                         '__MASKED__'),\n",
        "                                                              axis = 1)\n",
        "df['seq_length_mask_ess'] = [len(txt.split()) for txt in df['masked_ess_txt'].tolist()]\n",
        "df['masked_ess_txt'].head(), df['seq_length_mask_ess'].describe()"
      ],
      "metadata": {
        "id": "ZSq5YwUFHsgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to truncate discourse text and context text\n",
        "# this is still in progress\n",
        "def trunc_text(text, num_words, unique_centre_tok = None):\n",
        "    w_l = text.split()\n",
        "    if unique_centre_tok is None:\n",
        "        if len(w_l) > num_words: w_l = w_l[:num_words]\n",
        "    else:\n",
        "        if len(w_l) > num_words:\n",
        "            pos_tok = w_l.index(unique_centre_tok) + 1\n",
        "            if pos_tok > round(len(w_l) / 2):\n",
        "                start_pos = pos_tok - round(len(w_l) / 2) - 1 \n",
        "                w_l = w_l[start_pos:(start_pos + num_words)]\n",
        "                print(start_pos)\n",
        "            else:\n",
        "                w_l = w_l[:num_words]\n",
        "            \n",
        "    return ' '.join(w_l)\n",
        "\n",
        "trunc_text(\"let's see where we are __MASKED__ going.\", 5, unique_centre_tok = \"__MASKED__\")"
      ],
      "metadata": {
        "id": "VFLhCiRbrWKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all text columns for classification \n",
        "# concat all\n",
        "# df['all_text'] = 'CONTEXT: ' + df.essay_text + '; DISCOURSE: ' + df.discourse_text + '; TYPE: ' + df.discourse_type\n",
        "\n",
        "# concat after truncation\n",
        "df['essay_text_trunc'] = df.masked_ess_txt.apply(lambda t: trunc_text(t,512))\n",
        "df['discourse_text_trunc'] = df.discourse_text.apply(lambda t: trunc_text(t,64))\n",
        "df['all_text'] = 'CONTEXT: ' + df.essay_text_trunc + '; TYPE: ' + df.discourse_type + '; DISCOURSE: ' + df.discourse_text_trunc \n",
        "df['seq_length_all'] = [len(txt.split()) for txt in df['all_text'].tolist()]\n",
        "df['seq_length_all'].describe()"
      ],
      "metadata": {
        "id": "xy67RMN42OSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data loaders\n",
        "dls = TextDataLoaders.from_df(df, text_col='all_text',\n",
        "                              label_col='discourse_effectiveness',\n",
        "                              seq_len=650,\n",
        "                              text_vocab=dls_lm.vocab)\n",
        "dls.show_batch(max_n=3)"
      ],
      "metadata": {
        "id": "SpWa5edK5ac1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = text_classifier_learner(dls, AWD_LSTM, \n",
        "                                drop_mult=0.5,\n",
        "                                backwards=True,\n",
        "                                metrics=[accuracy,F1Score(average='weighted')]).to_fp16()\n",
        "# load encoder from language model\n",
        "if iskaggle: model_save_path = model_save_path / 'models'\n",
        "learn = learn.load_encoder(model_save_path / 'finetuned_enc')"
      ],
      "metadata": {
        "id": "yHF2TJZT74gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.lr_find(suggest_funcs=(valley, slide))"
      ],
      "metadata": {
        "id": "pxVOMs9zYHHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fine_tune(10, 0.01)"
      ],
      "metadata": {
        "id": "G2KpNDQf9FYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path.mkdir(parents=True, exist_ok=True)\n",
        "# learn.export(f'{model_save_path}/all_col_concat_learner.pkl')\n",
        "learn.save(f'{model_save_path}/all_text_concat_ulmfit_save.pkl')"
      ],
      "metadata": {
        "id": "Ax5UQ4TAPm8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test submission"
      ],
      "metadata": {
        "id": "xqlFEkilBkW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(path/'test.csv')\n",
        "# pre-process test df texts\n",
        "test_df['essay_text'] = test_df['essay_id'].apply(lambda x: file_read(path / 'test' / f'{x}.txt'))\n",
        "test_df['masked_ess_txt'] = test_df[['essay_text','discourse_text']].apply(lambda row: row.essay_text.strip().replace(row.discourse_text.strip(),\n",
        "                                                                                                         '__MASKED__'),\n",
        "                                                              axis = 1)\n",
        "test_df['essay_text_trunc'] = test_df.masked_ess_txt.apply(lambda t: trunc_text(t,512))\n",
        "test_df['discourse_text_trunc'] = test_df.discourse_text.apply(lambda t: trunc_text(t,64))\n",
        "test_df['text'] = 'CONTEXT: ' + test_df.essay_text_trunc + '; TYPE: ' + test_df.discourse_type + '; DISCOURSE: ' + test_df.discourse_text_trunc "
      ],
      "metadata": {
        "id": "gjJvftl_U8CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_df.rename(columns = {'discourse_text':'text'}, inplace = True)\n",
        "test_df.info()"
      ],
      "metadata": {
        "id": "WJszwQGq-hcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_dl = dls.test_dl(test_df)\n",
        "tst_dl.show_batch()"
      ],
      "metadata": {
        "id": "2nshwnkyCMXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\n",
        "probs"
      ],
      "metadata": {
        "id": "NU0WAzprCSja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# column names for probabilities\n",
        "probs_df = pd.DataFrame(probs.numpy(),columns = dls.vocab[1])\n",
        "probs_df"
      ],
      "metadata": {
        "id": "M9m21njOEF9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs_df[\"discourse_id\"] = test_df[\"discourse_id\"]\n",
        "probs_df.to_csv('submission.csv', index=False)\n",
        "!head submission.csv"
      ],
      "metadata": {
        "id": "WtpR0ynUCY-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not working for this competetion\n",
        "# if not iskaggle:\n",
        "#     from kaggle import api\n",
        "#     api.competition_submit_cli('submission.csv', 'initial', comp)"
      ],
      "metadata": {
        "id": "BCHJJtmoCkX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not iskaggle:\n",
        "    push_notebook('saan', comp,\n",
        "                  title='Feedback effeciveness with essay text using ulmfit-backward',\n",
        "                  file='/content/drive/MyDrive/Colab Notebooks/all_text_concat_ulmfit_model.ipynb',\n",
        "                  competition=comp, private=False, gpu=True)"
      ],
      "metadata": {
        "id": "3DYHPZlGaaV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rxb5R3vYbLaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}